{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42c9363a-bb96-42b0-ba8c-382a830f8ef4",
   "metadata": {},
   "source": [
    "# **1. Import Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f9eab9b5-7537-4f37-8511-c2dd48a923ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "import joblib\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c61df53-13b7-4d89-91f7-65ae803bd85b",
   "metadata": {},
   "source": [
    "# **2. Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5424b54a-12dc-451b-9ffd-6aa6a9e5f4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nama</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Ulasan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nia Devi</td>\n",
       "      <td>4 bintang</td>\n",
       "      <td>Dari sblm lahir, lahiran trs baby juga di rawa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>titania purnamasari</td>\n",
       "      <td>1 bintang</td>\n",
       "      <td>Gedung elit, parkir di basement sulit. Baru am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Farid Add</td>\n",
       "      <td>5 bintang</td>\n",
       "      <td>Sungguh luar biasa bagi saya untuk pelayanan t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Susi Mashanafi</td>\n",
       "      <td>1 bintang</td>\n",
       "      <td>Tes napza disini, udah selesai tinggal nunggu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Basuki Dwi Putranto</td>\n",
       "      <td>5 bintang</td>\n",
       "      <td>Selamat atas telah beroperasinya Gedung Baru K...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Nama     Rating  \\\n",
       "0             Nia Devi  4 bintang   \n",
       "1  titania purnamasari  1 bintang   \n",
       "2            Farid Add  5 bintang   \n",
       "3       Susi Mashanafi  1 bintang   \n",
       "4  Basuki Dwi Putranto  5 bintang   \n",
       "\n",
       "                                              Ulasan  \n",
       "0  Dari sblm lahir, lahiran trs baby juga di rawa...  \n",
       "1  Gedung elit, parkir di basement sulit. Baru am...  \n",
       "2  Sungguh luar biasa bagi saya untuk pelayanan t...  \n",
       "3  Tes napza disini, udah selesai tinggal nunggu ...  \n",
       "4  Selamat atas telah beroperasinya Gedung Baru K...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs = pd.read_csv(\"dataset/final-dataset.csv\", dtype=str) \n",
    "rs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91ade4b-17bf-4cc1-accf-890deffe9b8f",
   "metadata": {},
   "source": [
    "# **3. Memahami Struktur Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a51b2ba2-9248-4973-85c5-f44dad9e627a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Nama    3000 non-null   object\n",
      " 1   Rating  3000 non-null   object\n",
      " 2   Ulasan  3000 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 70.4+ KB\n"
     ]
    }
   ],
   "source": [
    "rs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495e56da-f255-430d-a898-f4cd46070548",
   "metadata": {},
   "source": [
    "# **4. Mengecheck data kosong**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8d37b662-9df5-4ba7-8322-d982f65abc3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nama      0\n",
       "Rating    0\n",
       "Ulasan    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mengecheck data hilang\n",
    "rs.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b62dcd-456f-410c-a526-f94a40d3dea5",
   "metadata": {},
   "source": [
    "# **5. Ekstraksi fitur dan pelabelan data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dcd42d27-d0ad-4578-a3e5-e1e5797b4629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\03ann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\03ann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "61b9b164-dc3b-4975-b8d7-cf857fa217d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_rating(rating):\n",
    "    if pd.isna(rating) or rating.strip() == \"\":  \n",
    "        return None\n",
    "    angka = re.findall(r\"\\d+\", rating) \n",
    "    return int(angka[0]) if angka else None \n",
    "\n",
    "rs[\"Rating\"] = rs[\"Rating\"].astype(str).apply(clean_rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1d8c4930-5f49-4c56-8813-b649b06334cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pastikan rating dalam bentuk string dulu\n",
    "rs[\"Rating\"] = rs[\"Rating\"].astype(str)\n",
    "\n",
    "# Ekstrak angka dari string \"5 bintang\", \"2 bintang\", dst\n",
    "rs[\"Rating\"] = rs[\"Rating\"].str.extract('(\\d)').astype(float)\n",
    "\n",
    "# Label sentimen\n",
    "def label_sentiment(rating):\n",
    "    if rating >= 4:\n",
    "        return \"Positif\"\n",
    "    else:\n",
    "        return \"Negatif\"\n",
    "\n",
    "rs[\"Sentimen\"] = rs[\"Rating\"].apply(label_sentiment)\n",
    "\n",
    "#data netral atau 3 dijadikan satu dengan negatif karena jumlah terlalu sedikit dan memperngaruhi pelatihan model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c8f80279-ac46-4926-81b6-f4275ccff594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentimen\n",
      "Negatif    1653\n",
      "Positif    1347\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(rs[\"Sentimen\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "de32c77f-1cca-4056-a6d6-30379705ce87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating\n",
      "1.0    1353\n",
      "5.0    1012\n",
      "4.0     335\n",
      "3.0     300\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(rs[\"Rating\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "81281d36-08a0-42d2-80d4-3cf0db21dcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Ulasan  \\\n",
      "2183  Kami sadar hanya pasien kelas 3 PBI. Tapi kami...   \n",
      "978   Sungguh luar biasa bagi saya untuk pelayanan t...   \n",
      "2743  Kami sadar hanya pasien kelas 3 PBI. Tapi kami...   \n",
      "2826  Sungguh luar biasa bagi saya untuk pelayanan t...   \n",
      "2662  Pelayanannya buruk sekali, pindah ke gedung ba...   \n",
      "\n",
      "                                           Ulasan_Clean  \n",
      "2183  kami sadar hanya pasien kelas pbi tapi kami ti...  \n",
      "978   sungguh luar biasa bagi saya untuk pelayanan t...  \n",
      "2743  kami sadar hanya pasien kelas pbi tapi kami ti...  \n",
      "2826  sungguh luar biasa bagi saya untuk pelayanan t...  \n",
      "2662  pelayanannya buruk_sekali pindah ke gedung bar...  \n"
     ]
    }
   ],
   "source": [
    "# Fungsi untuk membersihkan teks\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "    # Normalisasi slang / kata tidak baku\n",
    "    slang_map = {\n",
    "        \"ga\": \"tidak\",\n",
    "        \"gak\": \"tidak\",\n",
    "        \"nggak\": \"tidak\",\n",
    "        \"ngga\": \"tidak\",\n",
    "        \"cuman\": \"cuma\",\n",
    "        \"udh\": \"sudah\",\n",
    "        \"dr\": \"dari\",\n",
    "        \"bgt\": \"banget\",\n",
    "    }\n",
    "    for slang, formal in slang_map.items():\n",
    "        text = re.sub(rf'\\b{slang}\\b', formal, text)\n",
    "\n",
    "    # Gabungkan frasa negasi umum\n",
    "    negation_patterns = [\n",
    "        (r'\\btidak nyaman\\b', 'tidak_nyaman'),\n",
    "        (r'\\btidak enak\\b', 'tidak_enak'),\n",
    "        (r'\\btidak bagus\\b', 'tidak_bagus'),\n",
    "        (r'\\btidak bersih\\b', 'tidak_bersih'),\n",
    "        (r'\\btidak ramah\\b', 'tidak_ramah'),\n",
    "        (r'\\btidak kelar\\b', 'tidak_kelar'),\n",
    "        (r'\\btidak jelas\\b', 'tidak_jelas'),\n",
    "        (r'\\bburuk sekali\\b', 'buruk_sekali'),\n",
    "        (r'\\bjangan ke sini\\b', 'jangan_ke_sini'),\n",
    "    ]\n",
    "\n",
    "    for pattern, replacement in negation_patterns:\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "rs[\"Ulasan_Clean\"] = rs[\"Ulasan\"].astype(str).apply(clean_text)\n",
    "print(rs[[\"Ulasan\", \"Ulasan_Clean\"]].sample(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73893d31-a59f-4bef-aac1-b4990559b503",
   "metadata": {},
   "source": [
    "# *Tokenisasi Teks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "83bcfff3-aacb-436f-897d-ecc516c2908d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Nama', 'Rating', 'Ulasan', 'Sentimen', 'Ulasan_Clean'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(rs.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fafaeaf7-0822-490d-bd7f-0c4434e880fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6  33  34  35  36  37  11   2  38   2  39   6  40  41   3  42  43  44\n",
      "   45  46  47  48  49  50  12  51  52  17  13   4   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  7  53  14   2  54  55   8  56  57  14   2   7   8  58  59   3  60  61\n",
      "   62  63  14  64  65  66  67  68   4   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 18  19  20  69  21  12   9  22   2  23  70  24  25   6  71  72  73  21\n",
      "   74  75   5  76  77  78  79   9  10  18  19  20  80  81   4   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 82  83  84  85  86  87  88  89  10  26   3  90  26  91  92  93  94   3\n",
      "   95  11  96  97  98  99 100   4   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [101 102 103 104   7   8 105 106 107 108   9  10 109  12 110 111 112   5\n",
      "   24  22   2 113   5 114   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# Inisialisasi tokenizer\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(rs[\"Ulasan_Clean\"]) # Melatih tokenizer\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "\n",
    "# Konversi teks menjadi urutan angka\n",
    "sequences = tokenizer.texts_to_sequences(rs[\"Ulasan_Clean\"])\n",
    "# Padding sequences agar memiliki panjang yang sama\n",
    "max_length = 50  # Panjang maksimum ulasan\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=\"post\", truncating=\"post\", dtype=\"int32\")\n",
    "vocab_size = 5000\n",
    "# Cek hasil tokenisasi\n",
    "print(padded_sequences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8720ac44-f1df-48b9-82c2-2b1a93543b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Index: {'<OOV>': 1, 'di': 2, 'tidak': 3, '…': 4, 'dan': 5, 'dari': 6, 'gedung': 7, 'baru': 8, 'pelayanan': 9, 'yang': 10, 'juga': 11, 'untuk': 12, 'pasien': 13, 'parkir': 14, 'dengan': 15, 'kami': 16, 'ke': 17, 'sungguh': 18, 'luar': 19, 'biasa': 20, 'saya': 21, 'terutama': 22, 'bagian': 23, 'anak': 24, 'karena': 25, 'katanya': 26, 'ramah': 27, 'hanya': 28, 'saja': 29, 'antrian': 30, 'pelayanannya': 31, 'ada': 32, 'sblm': 33, 'lahir': 34, 'lahiran': 35, 'trs': 36, 'baby': 37, 'rawat': 38, 'nicu': 39, 'segi': 40, 'fasilitas': 41, 'ragu': 42, 'cmn': 43, 'beberapa': 44, 'dokter': 45, 'residen': 46, 'yg': 47, 'jaga': 48, 'kurang': 49, 'informatif': 50, 'kasi': 51, 'penjelasan': 52, 'elit': 53, 'basement': 54, 'sulit': 55, 'ambil': 56, 'karcis': 57, 'motor': 58, 'parkirnya': 59, 'jelas': 60, 'ngehalangi': 61, 'jalan': 62, 'petugas': 63, 'cuma': 64, 'orang': 65, 'duduk': 66, 'diplang': 67, 'keluar': 68, 'bagi': 69, 'bedah': 70, 'agustus': 71, 'sampai': 72, 'pebruari': 73, 'merasakan': 74, 'kenyamananketenangan': 75, 'bahagia': 76, 'itu': 77, 'semua': 78, 'berkat': 79, 'penilaian': 80, 'dah': 81, 'tes': 82, 'napza': 83, 'disini': 84, 'udah': 85, 'selesai': 86, 'tinggal': 87, 'nunggu': 88, 'hasil': 89, 'lama': 90, 'biasanya': 91, 'menit': 92, 'realitanya': 93, 'jam': 94, 'kelar': 95, 'anjir': 96, 'very': 97, 'waste': 98, 'of': 99, 'time': 100, 'selamat': 101, 'atas': 102, 'telah': 103, 'beroperasinya': 104, 'kia': 105, 'semoga': 106, 'dapat': 107, 'memberikan': 108, 'terbaik': 109, 'peningkatan': 110, 'kesehatan': 111, 'ibu': 112, 'bali': 113, 'sekitarnya': 114, 'bagus': 115, 'staf': 116, 'dokternya': 117, 'kita': 118, 'perlu': 119, 'lebih': 120, 'bersabar': 121, 'pasiennya': 122, 'lumayan': 123, 'banyak': 124, 'buruk': 125, 'sekali': 126, 'pindah': 127, 'sama': 128, 'diperbaiki': 129, 'saat': 130, 'tanya': 131, 'informasi': 132, 'sopan': 133, 'santunnya': 134, 'jutek': 135, 'ketus': 136, 'staff': 137, 'register': 138, 'sempetnya': 139, 'main': 140, 'game': 141, 'sadar': 142, 'kelas': 143, 'pbi': 144, 'tapi': 145, 'sanggup': 146, 'kalau': 147, 'dianggap': 148, 'benalu': 149, 'dirujuk': 150, 'faskes': 151, 'type': 152, 'c': 153, 'ruang': 154, 'bersih': 155, 'ber': 156, 'ac': 157, 'monitor': 158, 'folded': 159, 'hands': 160, 'thumbs': 161, 'up': 162, 'light': 163, 'skin': 164, 'tone': 165}\n"
     ]
    }
   ],
   "source": [
    "print(\"Word Index:\", tokenizer.word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8d79496c-e90d-4d08-883f-b917a004aae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape TF-IDF Matrix: (3000, 353)\n",
      "Kata-kata dalam TF-IDF: ['ac' 'ac dengan' 'ada' 'ada ramah' 'ada yang' 'agustus' 'agustus sampai'\n",
      " 'ambil' 'ambil karcis' 'anak' 'anak karena' 'anak terutama' 'anjir'\n",
      " 'anjir very' 'antrian' 'antrian karena' 'atas' 'atas telah' 'baby'\n",
      " 'baby juga' 'bagi' 'bagi saya' 'bagian' 'bagian bedah' 'bagian register'\n",
      " 'bagus' 'bagus staf' 'bahagia' 'bahagia itu' 'bali' 'bali dan' 'banyak'\n",
      " 'baru' 'baru ambil' 'baru juga' 'baru kia' 'baru motor' 'basement'\n",
      " 'basement sulit' 'beberapa' 'beberapa dokter' 'bedah' 'bedah anak'\n",
      " 'benalu' 'benalu kami' 'ber' 'ber ac' 'berkat' 'berkat pelayanan'\n",
      " 'beroperasinya']\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, min_df=1, max_df=0.95,ngram_range=(1,2))  \n",
    "X_tfidf = tfidf_vectorizer.fit_transform(rs[\"Ulasan_Clean\"])\n",
    "\n",
    "# Cek ukuran data\n",
    "print(\"Shape TF-IDF Matrix:\", X_tfidf.shape)\n",
    "print(\"Kata-kata dalam TF-IDF:\", tfidf_vectorizer.get_feature_names_out()[:50])  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9a2b4aa0-682b-4154-ba2d-40ea62d2623d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape TF-IDF Matrix: (3000, 353)\n"
     ]
    }
   ],
   "source": [
    "X_tfidf = tfidf_vectorizer.fit_transform(rs[\"Ulasan_Clean\"])\n",
    "print(\"Shape TF-IDF Matrix:\", X_tfidf.shape)\n",
    "y = rs[\"Sentimen\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d424b26d-3e6d-453e-9390-eb3bf6b8bce7",
   "metadata": {},
   "source": [
    "# **6. Pelatihan machine learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f12cd3a3-966e-4bcf-ba60-5f7eac0a5e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pembagian data 80/20 dan 70/30\n",
    "X_train_80, X_test_80, y_train_80, y_test_80 = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "X_train_70, X_test_70, y_train_70, y_test_70 = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2e314e33-2c2f-481f-9040-2141a236476e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Random Forest (80/20) Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negatif       1.00      0.90      0.95       344\n",
      "     Positif       0.89      1.00      0.94       256\n",
      "\n",
      "    accuracy                           0.94       600\n",
      "   macro avg       0.94      0.95      0.94       600\n",
      "weighted avg       0.95      0.94      0.95       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Skema 1: Random Forest dengan split 80/20\n",
    "model_rf = RandomForestClassifier(random_state=42)\n",
    "model_rf.fit(X_train_80, y_train_80)\n",
    "y_pred_rf = model_rf.predict(X_test_80)\n",
    "print(\"\\n🎯 Random Forest (80/20) Classification Report:\")\n",
    "print(classification_report(y_test_80, y_pred_rf, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "33a64e9e-e651-49c8-a6e9-2174cca883c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 SVM (70/30) Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negatif       1.00      0.90      0.95       500\n",
      "     Positif       0.89      1.00      0.94       400\n",
      "\n",
      "    accuracy                           0.95       900\n",
      "   macro avg       0.95      0.95      0.95       900\n",
      "weighted avg       0.95      0.95      0.95       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Skema 2: SVM dengan split 70/30\n",
    "model_svm = SVC(kernel='linear', probability=True)\n",
    "model_svm.fit(X_train_70, y_train_70)\n",
    "y_pred_svm = model_svm.predict(X_test_70)\n",
    "print(\"\\n🎯 SVM (70/30) Classification Report:\")\n",
    "print(classification_report(y_test_70, y_pred_svm, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0041078-862d-4744-b9f8-36c21f19ba2c",
   "metadata": {},
   "source": [
    "# **7. Pelatihan model deep learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "01baaeeb-3206-45e4-bd5c-b63eb7004bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label mapping dari sebelumnya\n",
    "label_mapping = {'Negatif': 0, 'Positif': 1}\n",
    "labels = rs[\"Sentimen\"].map(label_mapping).values\n",
    "labels_encoded = to_categorical(labels, num_classes=3).astype(\"float32\")\n",
    "\n",
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(\n",
    "    padded_sequences, labels_encoded, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1a35e2df-0e96-49ab-94fa-7be0d4858719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 50, 128)           640000    \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 50, 64)            49408     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 50, 64)            0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 722627 (2.76 MB)\n",
      "Trainable params: 722627 (2.76 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "60/60 [==============================] - 14s 89ms/step - loss: 0.5181 - accuracy: 0.7563 - val_loss: 0.1478 - val_accuracy: 0.9583\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - 3s 52ms/step - loss: 0.1771 - accuracy: 0.9479 - val_loss: 0.1444 - val_accuracy: 0.9583\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - 3s 53ms/step - loss: 0.1798 - accuracy: 0.9479 - val_loss: 0.1431 - val_accuracy: 0.9583\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - 3s 51ms/step - loss: 0.1748 - accuracy: 0.9479 - val_loss: 0.1459 - val_accuracy: 0.9583\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - 3s 53ms/step - loss: 0.1765 - accuracy: 0.9479 - val_loss: 0.1462 - val_accuracy: 0.9583\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - 3s 54ms/step - loss: 0.1759 - accuracy: 0.9479 - val_loss: 0.1427 - val_accuracy: 0.9583\n",
      "Epoch 7/10\n",
      "60/60 [==============================] - 3s 52ms/step - loss: 0.1732 - accuracy: 0.9479 - val_loss: 0.1427 - val_accuracy: 0.9583\n",
      "Epoch 8/10\n",
      "60/60 [==============================] - 3s 53ms/step - loss: 0.1739 - accuracy: 0.9479 - val_loss: 0.1429 - val_accuracy: 0.9583\n",
      "Epoch 9/10\n",
      "60/60 [==============================] - 3s 52ms/step - loss: 0.1739 - accuracy: 0.9479 - val_loss: 0.1427 - val_accuracy: 0.9583\n",
      "Epoch 10/10\n",
      "60/60 [==============================] - 3s 53ms/step - loss: 0.1720 - accuracy: 0.9479 - val_loss: 0.1422 - val_accuracy: 0.9583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x18fa566be50>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "lstm_units = 64\n",
    "\n",
    "model_lstm = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=128, input_length=50),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64),\n",
    "    Dropout(0.2),\n",
    "    Dense(3, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "\n",
    "model_lstm.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model_lstm.summary()\n",
    "y_int = rs[\"Sentimen\"].map(label_mapping).values\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_int), y=y_int)\n",
    "\n",
    "# Training\n",
    "model_lstm.fit(X_train_lstm, y_train_lstm, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "535876fa-3fe4-40d6-8ee7-e642685fe3f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 3s 19ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Negatif       1.00      0.90      0.95       344\n",
      "     Positif       0.89      1.00      0.94       256\n",
      "\n",
      "    accuracy                           0.94       600\n",
      "   macro avg       0.94      0.95      0.94       600\n",
      "weighted avg       0.95      0.94      0.95       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_lstm.predict(X_test_lstm)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test_lstm, axis=1)\n",
    "\n",
    "print(classification_report(y_true, y_pred_labels, target_names=[\"Negatif\", \"Positif\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9920d316-7dcd-4435-b340-2f7cd41f86b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, tokenizer, model, max_length=50, threshold=0.5):\n",
    "    # Step 1: Bersihkan teks\n",
    "    text_clean = clean_text(text)\n",
    "    \n",
    "    # Step 2: Tokenisasi & Padding\n",
    "    seq = tokenizer.texts_to_sequences([text_clean])\n",
    "    \n",
    "    if len(seq[0]) == 0:\n",
    "        return \"Teks tidak dikenali oleh model (hasil tokenisasi kosong).\"\n",
    "    \n",
    "    pad_seq = pad_sequences(seq, maxlen=max_length, padding='post')\n",
    "    \n",
    "    # Step 3: Prediksi\n",
    "    pred = model.predict(pad_seq, verbose=0)\n",
    "    label_index = np.argmax(pred)\n",
    "    confidence = np.max(pred)\n",
    "\n",
    "    # Step 4: Mapping label\n",
    "    reverse_label_mapping = {0: \"Negatif\", 1: \"Positif\"}\n",
    "    predicted_label = reverse_label_mapping[label_index]\n",
    "\n",
    "    # Step 5: Output + Confidence Check\n",
    "    if confidence < threshold:\n",
    "        return f\"Model kurang yakin (confidence: {confidence:.2f}). Prediksi: {predicted_label}\"\n",
    "    else:\n",
    "        return f\"{predicted_label} (confidence: {confidence:.2f})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "01dc1489-3d4e-4c0a-9a54-acab1e5d8d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positif (confidence: 0.92)'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"pelayanan bagus staf dan dokternya ramah\", tokenizer, model_lstm)\n",
    "# Output: Positif \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3881c48b-33ea-49b8-8028-71bfcf6dbaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negatif (confidence: 1.00)'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"tidak ada ramah sopan santunnya jutek ketus dan staff di bagian register main game doang, buruk sekali\", tokenizer, model_lstm)\n",
    "# Output: Negatif\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dea5ea3-996e-4e5a-966a-1213ddad498d",
   "metadata": {},
   "source": [
    "# **8. Export pkl**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "45de511c-fbd2-4f08-909c-7fffc47b835d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(tfidf_vectorizer, \"tfidf_vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6771f59e-3168-4019-8db8-5f324be16307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
